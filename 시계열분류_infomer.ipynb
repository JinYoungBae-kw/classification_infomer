{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import math\n",
        "from math import sqrt"
      ],
      "metadata": {
        "id": "DJ_a-4H3bov0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDM7SL0EGFAX"
      },
      "outputs": [],
      "source": [
        "class ProbAttention(nn.Module):\n",
        "    def __init__(self, factor=5, scale=None, attention_dropout=0.1, output_attention=False):\n",
        "        super(ProbAttention, self).__init__()\n",
        "        self.factor = factor\n",
        "        self.scale = scale\n",
        "        self.output_attention = output_attention\n",
        "        self.dropout = nn.Dropout(attention_dropout)\n",
        "\n",
        "    def _prob_QK(self, Q, K, sample_k, n_top):\n",
        "        B, H, L_K, E = K.shape\n",
        "        _, _, L_Q, _ = Q.shape\n",
        "\n",
        "        K_expand = K.unsqueeze(-3).expand(B, H, L_Q, L_K, E)\n",
        "        index_sample = torch.randint(low=0, high=L_K, size=(L_Q, sample_k), device=Q.device)\n",
        "        K_sample = K_expand[:, :, torch.arange(L_Q).unsqueeze(1), index_sample, :]\n",
        "        Q_K_sample = torch.matmul(Q.unsqueeze(-2), K_sample.transpose(-2, -1)).squeeze(-2)\n",
        "\n",
        "        M = Q_K_sample.max(-1)[0] - torch.div(Q_K_sample.sum(-1), L_K)\n",
        "        M_top = M.topk(n_top, sorted=False)[1]\n",
        "\n",
        "        Q_reduce = Q[torch.arange(B)[:, None, None], torch.arange(H)[None, :, None], M_top, :]\n",
        "        Q_K = torch.matmul(Q_reduce, K.transpose(-2, -1))\n",
        "\n",
        "        return Q_K, M_top\n",
        "\n",
        "    def _get_initial_context(self, V, L_Q):\n",
        "        B, H, L_V, D = V.shape\n",
        "\n",
        "        V_sum = V.mean(dim=-2)\n",
        "        contex = V_sum.unsqueeze(-2).expand(B, H, L_Q, V_sum.shape[-1]).clone()\n",
        "\n",
        "        return contex\n",
        "\n",
        "    def _update_context(self, context_in, V, scores, index):\n",
        "        B, H, L_V, D = V.shape\n",
        "\n",
        "        attn = torch.softmax(scores, dim=-1) # nn.Softmax(dim=-1)(scores)\n",
        "        context_in[torch.arange(B)[:, None, None],\n",
        "                   torch.arange(H)[None, :, None],\n",
        "                   index, :] = torch.matmul(attn, V).type_as(context_in)\n",
        "\n",
        "        if self.output_attention:\n",
        "            attns = (torch.ones([B, H, L_V, L_V])/L_V).type_as(attn).to(attn.device)\n",
        "            attns[torch.arange(B)[:, None, None], torch.arange(H)[None, :, None], index, :] = attn\n",
        "            return (context_in, attns)\n",
        "        else:\n",
        "            return (context_in, None)\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        B, L_Q, H, D = queries.shape\n",
        "        _, L_K, _, _ = keys.shape\n",
        "\n",
        "        queries = queries.transpose(2,1)\n",
        "        keys = keys.transpose(2,1)\n",
        "        values = values.transpose(2,1)\n",
        "\n",
        "        U_part = self.factor * np.ceil(np.log(L_K)).astype('int').item()\n",
        "        u = self.factor * np.ceil(np.log(L_Q)).astype('int').item()\n",
        "        U_part = U_part if U_part<L_K else L_K\n",
        "        u = u if u<L_Q else L_Q\n",
        "\n",
        "        scores_top, index = self._prob_QK(queries, keys, sample_k=U_part, n_top=u)\n",
        "        scale = 1./sqrt(D)\n",
        "        scores_top = scores_top * scale\n",
        "        context = self._get_initial_context(values, L_Q)\n",
        "        context, attn = self._update_context(context, values, scores_top, index)\n",
        "\n",
        "        return context.transpose(2,1).contiguous(), attn\n",
        "\n",
        "\n",
        "class AttentionLayer(nn.Module):\n",
        "    def __init__(self, attention, d_model, n_heads, d_keys=None, d_values=None):\n",
        "        super(AttentionLayer, self).__init__()\n",
        "\n",
        "        d_keys = d_keys or (d_model//n_heads)\n",
        "        d_values = d_values or (d_model//n_heads)\n",
        "\n",
        "        self.inner_attention = attention\n",
        "        self.query_projection = nn.Linear(d_model, d_keys * n_heads)\n",
        "        self.key_projection = nn.Linear(d_model, d_keys * n_heads)\n",
        "        self.value_projection = nn.Linear(d_model, d_values * n_heads)\n",
        "        self.out_projection = nn.Linear(d_values * n_heads, d_model)\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        B, L, _ = queries.shape\n",
        "        _, S, _ = keys.shape\n",
        "        H = self.n_heads\n",
        "\n",
        "        queries = self.query_projection(queries).view(B, L, H, -1)\n",
        "        keys = self.key_projection(keys).view(B, S, H, -1)\n",
        "        values = self.value_projection(values).view(B, S, H, -1)\n",
        "\n",
        "        out, attn = self.inner_attention(\n",
        "            queries,\n",
        "            keys,\n",
        "            values\n",
        "        )\n",
        "        out = out.view(B, L, -1)\n",
        "\n",
        "        return self.out_projection(out), attn\n",
        "\n",
        "class ConvLayer(nn.Module):\n",
        "    def __init__(self, c_in):\n",
        "        super(ConvLayer, self).__init__()\n",
        "        padding = 1 if torch.__version__>='1.5.0' else 2\n",
        "        self.downConv = nn.Conv1d(in_channels=c_in,\n",
        "                                  out_channels=c_in,\n",
        "                                  kernel_size=3,\n",
        "                                  padding=padding,\n",
        "                                  padding_mode='circular')\n",
        "        self.norm = nn.BatchNorm1d(c_in)\n",
        "        self.activation = nn.ELU()\n",
        "        self.maxPool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.downConv(x.permute(0, 2, 1))\n",
        "        x = self.norm(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.maxPool(x)\n",
        "        x = x.transpose(1,2)\n",
        "        return x\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, attention, d_model, d_ff=None, dropout=0.1, activation=\"relu\"):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        d_ff = d_ff or 4*d_model\n",
        "        self.attention = attention\n",
        "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n",
        "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.activation = F.relu if activation == \"relu\" else F.gelu\n",
        "\n",
        "    def forward(self, x):\n",
        "        new_x, attn = self.attention(\n",
        "            x, x, x\n",
        "        )\n",
        "        x = x + self.dropout(new_x)\n",
        "        y = x = self.norm1(x)\n",
        "        y = self.dropout(self.activation(self.conv1(y.transpose(-1,1))))\n",
        "        y = self.dropout(self.conv2(y).transpose(-1,1))\n",
        "\n",
        "        return self.norm2(x+y), attn\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, attn_layers, conv_layers=None, norm_layer=None):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.attn_layers = nn.ModuleList(attn_layers)\n",
        "        self.conv_layers = nn.ModuleList(conv_layers) if conv_layers is not None else None\n",
        "        self.norm = norm_layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        attns = []\n",
        "\n",
        "        for attn_layer, conv_layer in zip(self.attn_layers, self.conv_layers):\n",
        "            x, attn = attn_layer(x)\n",
        "            x = conv_layer(x)\n",
        "            attns.append(attn)\n",
        "\n",
        "        x, attn = self.attn_layers[-1](x)\n",
        "        attns.append(attn)\n",
        "        x = self.norm(x)\n",
        "\n",
        "        return x, attns\n",
        "\n",
        "class JointEmbedding(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super(JointEmbedding, self).__init__()\n",
        "        self.embedding = nn.Linear(2, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, J = x.shape\n",
        "        num_joints = J // 2\n",
        "        x = x.view(B, T, num_joints, 2)\n",
        "        x = self.embedding(x)\n",
        "        x = x.view(B, T, -1)\n",
        "        return x\n",
        "\n",
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEmbedding, self).__init__()\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model).float()\n",
        "        pe.require_grad = False\n",
        "\n",
        "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
        "        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.pe[:, :x.size(1)]\n",
        "\n",
        "class DataEmbedding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1):\n",
        "        super(DataEmbedding, self).__init__()\n",
        "\n",
        "        self.value_embedding = JointEmbedding(d_model=d_model)\n",
        "        self.position_embedding = PositionalEmbedding(d_model=d_model)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.value_embedding(x) + self.position_embedding(x)\n",
        "\n",
        "        return self.dropout(x)\n",
        "\n",
        "class Informer(nn.Module):\n",
        "    def __init__(self, c_out, factor=5, d_model=512, n_heads=8, e_layers=3, d_ff=512, dropout=0.0,\n",
        "                 activation='gelu',output_attention = False, device=torch.device('cuda:0')):\n",
        "        super(Informer, self).__init__()\n",
        "        self.output_attention = output_attention\n",
        "\n",
        "        self.enc_embedding = DataEmbedding(d_model, dropout)\n",
        "        Attn = ProbAttention\n",
        "        self.encoder = Encoder(\n",
        "            [\n",
        "                EncoderLayer(\n",
        "                    AttentionLayer(Attn(factor, attention_dropout=dropout, output_attention=output_attention),\n",
        "                                d_model, n_heads),\n",
        "                    d_model,\n",
        "                    d_ff,\n",
        "                    dropout=dropout,\n",
        "                    activation=activation\n",
        "                ) for l in range(e_layers)\n",
        "            ],\n",
        "            [\n",
        "                ConvLayer(\n",
        "                    d_model\n",
        "                ) for l in range(e_layers-1)\n",
        "            ],\n",
        "            norm_layer=torch.nn.LayerNorm(d_model)\n",
        "        )\n",
        "        self.projection = nn.Linear(d_model, c_out, bias=True)\n",
        "\n",
        "    def forward(self, x_enc):\n",
        "        enc_out = self.enc_embedding(x_enc)\n",
        "        enc_out, attns = self.encoder(enc_out)\n",
        "\n",
        "        out = self.projection(enc_out[:, -1, :])\n",
        "\n",
        "        return out"
      ]
    }
  ]
}